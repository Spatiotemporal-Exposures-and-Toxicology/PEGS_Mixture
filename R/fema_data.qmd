---
title: "FEMA disaster aid data"
author: Insang Song
date: 12/14/2023
output:
  html:
    toc: true
    style: cerulean
---



```{r}
sysinfo <- Sys.info()
if (sysinfo["sysname"] == "Linux") {
  if (dir.exists("~/r-libs")) {
    .libPaths("~/r-libs")
  }
  pythonpath <- "/ddn/gs1/biotools/python3/bin/python3"
}


knitr::opts_chunk$set(echo = FALSE)
reticulate::use_python(pythonpath)
reticulate::virtualenv_create(
    envname = "pegs-isong",
    python = pythonpath,
    packages = c("dask_geopandas", "dask", "pypolars", "xarray", "rioxarray"))

```




```{python}
import dask_geopandas as dgpd
import dask.dataframe as ddf

basedir = "/ddn/gs1/home/songi2/projects/PEGS_Mixture/input/fema/"

fema_reg_rent = ddf.read_csv(basedir + "HousingAssistanceRenters.csv", \
    dtype = {'zipCode': 'object'})
fema_reg_own = ddf.read_csv(basedir + "HousingAssistanceOwners.csv", \
    dtype = {'zipCode': 'object'})
fema_reg_dec = ddf.read_csv(basedir + "DisasterDeclarationsSummaries.csv", \
    dtype = {'lastIAFilingDate': 'object'})
fema_ind_house = ddf.read_csv(basedir + "IndividualAssistanceHousingRegistrantsLargeDisasters.csv", \
    dtype={'censusBlockId': 'float64',
       'censusYear': 'float64',
       'primaryResidence': 'float64',
       'rentalAssistanceEndDate': 'str',
       'rentalResourceCity': 'str',
       'rentalResourceStateAbbreviation': 'str',
       'rentalResourceZipCode': 'str',
       'renterDamageLevel': 'str'})
fema_ind_valid = ddf.read_csv(basedir + "IndividualsAndHouseholdsProgramValidRegistrations.csv", \
    dtype={'autoDamage': 'float64',
       'damagedZipCode': 'float64',
       'primaryResidence': 'float64',
       'incidentType': 'str',
       'habitabilityRepairsRequired': 'float64',
       'homeDamage': 'float64',
       'householdComposition': 'str',
       'occupants19to64': 'str',
       'occupants6to18': 'str',
       'registrationMethod': 'str'})


fema_reg_own.head()
fema_reg_rent.head()
fema_reg_dec.head()
fema_ind_house.head()
fema_ind_valid.head()

kk = fema_ind_valid.incidentType.unique()
kk.compute()
kk = fema_ind_valid.incidentType
kk.compute()

fema_reg_dec.sort_values("declarationDate", ascending = True).compute()

fema_reg_rent.sort_values("totalMaxGrants").compute()
fema_reg_rent.columns

fema = ddf.read_csv("~/Downloads/IndividualAssistanceHousingRegistrantsLargeDisasters.csv", \
    dtype={'censusBlockId': 'float64',
        'censusYear': 'float64',
        'primaryResidence': 'float64',
        'rentalAssistanceEndDate': 'object',
        'rentalResourceCity': 'object',
        'rentalResourceStateAbbreviation': 'object',
        'rentalResourceZipCode': 'object',
        'renterDamageLevel': 'object'})

fema.head()
fema.columns

fema_years = fema.censusYear.unique()
fema_years.compute()

```


```{r data-explore-r}
pkgs <- c("data.table", "tidytable", "ggplot2")
invisible(sapply(pkgs, library, character.only = TRUE, quietly = TRUE))
filename <- "./input/fema/IndividualsAndHouseholdsProgramValidRegistrations.csv"
femaindiv <- fread(filename)


# by state
femaindiv_summary <-
    femaindiv |>
    mutate(year = year(declarationDate)) |>
    group_by(year, damagedStateAbbreviation, incidentType, haStatus) |>
    summarize(
        n = n(),
        medianhaAmount = median(haAmount[haEligible == 1]),
        meanwaterLevel = mean(waterLevel[floodDamage == 1]),
        sumpropertyAmount = sum(personalPropertyAmount[personalPropertyEligible == 1])
    ) |>
    ungroup()
femaindiv_summary

femaindiv_summary_gg <-
    ggplot(data = femaindiv_summary[femaindiv_summary$damagedStateAbbreviation == "NC", ],
           mapping = aes(x = year, group = haStatus)) +
    stat_summary(mapping = aes(y = sumpropertyAmount), geom = "line", fun = sum, na.rm = TRUE, color = 'red') +
    # stat_summary(mapping = aes(y = n), geom = "line", fun = sum, na.rm = TRUE, color = 'green') +
    facet_wrap(~incidentType)
femaindiv_summary_gg


femaindiv |>
    filter(grepl("Flood", incidentType) & damagedStateAbbreviation == "NC")
femaindiv |>
    filter(grepl("Severe Storm", incidentType) & damagedStateAbbreviation == "NC")
femaindiv |>
    filter(grepl("Tornado", incidentType) & damagedStateAbbreviation == "NC")

femaindiv |>
    filter(incidentType == "Tornado") |>
    _[, damagedStateAbbreviation] |>
    unique()
femaindiv$incidentType |> unique()

```

Not many cases were reported in flood/tornados. Hurricane and severe storm categories reported some (but not many) damages in FEMA database.

Let's move on to the NOAA storm events.


```{r noaa-storm-r-download}
library(rvest)
library(terra)

# https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/StormEvents_details-ftp_v1.0_d2015_c20220425.csv.gz
noaacsvlinks <-
read_html("https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/") |>
    html_table()

truelinks <-
    sprintf("%s/%s",
    "https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles",
    grep("d[2][0][0-9]{2,2}", noaacsvlinks[[1]]$Name, value = TRUE))
truelinks

# sapply(truelinks, \(x) {
#     system(sprintf("wget -P ./input/noaastorm %s", x))
#     Sys.sleep(0.5)
# })

gzfiles <- list.files("./input/noaastorm", "*.csv.gz$", full.names = TRUE)
sapply(gzfiles, \(x) {
    system(sprintf("gzip -d %s", x))
})


```


```{r noaa-storm-cleaning}
csvfiles <- list.files("./input/noaastorm", "*.csv$", full.names = TRUE)

csv_locs <- grep("location", csvfiles, value = TRUE)
csv_fatal <- grep("fatalit", csvfiles, value = TRUE)
csv_detail <- grep("details", csvfiles, value = TRUE)

dt_locs <- lapply(csv_locs, data.table::fread) |>
    data.table::rbindlist()
dt_fatal <- lapply(csv_fatal, data.table::fread) |>
    data.table::rbindlist()
dt_detail <- lapply(csv_detail, data.table::fread) |>
    data.table::rbindlist()

dt_detail_clear <-
    dt_detail |>
    transmute(
        date = sprintf("%d%02d%04d", END_YEARMONTH, END_DAY, END_TIME),
        episode_id = EPISODE_ID,
        event_id = EVENT_ID,
        event_type = EVENT_TYPE,
        state = STATE,
        begin_lat = BEGIN_LAT,
        begin_lon = BEGIN_LON,
        end_lat = END_LAT,
        end_lon = END_LON
    )


dt_locs_clear <-
    dt_locs |>
    transmute(
        episode_id = EPISODE_ID,
        event_id = EVENT_ID,
        range_mi = RANGE,
        latitude = LATITUDE,
        longitude = LONGITUDE
    )

dt_locdetail <-
    full_join(dt_detail_clear, dt_locs_clear,
        by = c("episode_id", "event_id"))

dt_locdetail_vect <-
    dt_locdetail |>
    filter(!is.na(latitude)) |>
    data.frame() |>
    vect(geom = c("longitude", "latitude"), crs = "EPSG:4326")

mainland_ext <- ext(c(xmin = -128, xmax = -64, ymin = 20, ymax = 52))

dt_locdetail_ml <- dt_locdetail_vect[mainland_ext, ]
dt_locdetail_ml19 <- dt_locdetail_ml[substr(dt_locdetail_ml$date, 1, 4) == "2019", ]

dt_locdetail_ml19s <-
    split(dt_locdetail_ml19, dt_locdetail_ml19$range_mi == 0)
dt_locdetail_ml19b <-
terra::buffer(dt_locdetail_ml19s[[1]], 1609 * dt_locdetail_ml19s[[1]]$range_mi)


plot(dt_locdetail_ml19, y = "event_type", type = "classes", cex = 0.33)

```